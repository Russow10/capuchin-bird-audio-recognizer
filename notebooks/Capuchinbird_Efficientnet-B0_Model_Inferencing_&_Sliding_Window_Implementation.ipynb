{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capuchin Bird Call Detection System\n",
    "\n",
    "This notebook implements an efficient deep learning system for detecting Capuchin bird calls in forest audio recordings. The system utilizes a pre-trained EfficientNet-B0 convolutional neural network combined with a two-stage sliding window algorithm for optimized inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Overview\n",
    "\n",
    "The Capuchin bird detection system processes audio recordings through:\n",
    "1. **Feature Extraction**: Converting audio to mel spectrograms\n",
    "2. **Two-Stage Detection**: \n",
    "   - First stage: Quick screening of large audio segments (6-second windows)\n",
    "   - Second stage: Detailed analysis of promising segments\n",
    "3. **Classification**: Using an EfficientNet-B0 model to classify audio segments\n",
    "\n",
    "This approach balances efficiency and accuracy by only performing detailed analysis on segments that show potential bird calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "The `Config` class centralizes all system parameters including:\n",
    "- **Audio Processing**: Sample rate (16kHz), mel bands (128), FFT settings\n",
    "- **Detection Thresholds**: Stage 1 (0.6) and Stage 2 (0.5) confidence thresholds\n",
    "- **Window Parameters**: Outer window duration (6s), inner step duration (0.3s)\n",
    "- **File Paths**: Data locations, model weights, and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.applications import EfficientNetB0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "The `Config` class centralizes all system parameters including:\n",
    "- **Audio Processing**: Sample rate (16kHz), mel bands (128), FFT settings\n",
    "- **Detection Thresholds**: Stage 1 (0.6) and Stage 2 (0.5) confidence thresholds\n",
    "- **Window Parameters**: Outer window duration (6s), inner step duration (0.3s)\n",
    "- **File Paths**: Data locations, model weights, and output paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration class for Capuchin bird call detection.\n",
    "    \n",
    "    This class centralizes all settings and parameters used for inference,\n",
    "    including paths, audio processing parameters, and detection thresholds.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Paths\n",
    "    REAL_WORLD_DATA_FOLDER = 'data/Forest Recordings'\n",
    "    MANUAL_COUNTS_CSV_PATH = 'answer/capuchin_correct_call_counts.csv'\n",
    "    MODEL_WEIGHTS_FILE = 'weights/capuchin_bird_classifier.weights.h5'\n",
    "    \n",
    "    # Audio processing parameters\n",
    "    TARGET_SR = 16000\n",
    "    N_MELS = 128\n",
    "    N_FFT = 2048\n",
    "    HOP_LENGTH = 512\n",
    "    RES_TYPE = 'kaiser_best'\n",
    "    \n",
    "    # Sliding window parameters\n",
    "    THRESHOLD_STAGE1 = 0.6\n",
    "    THRESHOLD_STAGE2 = 0.5\n",
    "    WINDOW_DURATION_OUTER = 6.0\n",
    "    STEP_DURATION_INNER = 0.3\n",
    "    OVERLAP_INNER = 0.0\n",
    "    \n",
    "    # Input shape for the model\n",
    "    INPUT_SHAPE = (128, 157, 3)\n",
    "    \n",
    "    @classmethod\n",
    "    def setup(cls):\n",
    "        \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "        seed_value = 42\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "        random.seed(seed_value)\n",
    "        np.random.seed(seed_value)\n",
    "        tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "source": [
    "Config.setup()\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Audio signals are converted to mel spectrograms - a time-frequency representation that captures the spectral content of audio in a way that approximates human auditory perception. These spectrograms are then processed into the appropriate format for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(audio_path, target_sr=Config.TARGET_SR, n_mels=Config.N_MELS, \n",
    "                           n_fft=Config.N_FFT, hop_length=Config.HOP_LENGTH, \n",
    "                           y=None, sr=None, target_time_frames=None):\n",
    "    \"\"\"\n",
    "    Extract Mel spectrogram features from audio.\n",
    "    \n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        target_sr (int): Target sample rate for audio.\n",
    "        n_mels (int): Number of Mel bands to generate.\n",
    "        n_fft (int): Length of the FFT window.\n",
    "        hop_length (int): Number of samples between successive frames.\n",
    "        y (np.ndarray, optional): Audio time series.\n",
    "        sr (int, optional): Sample rate of y.\n",
    "        target_time_frames (int, optional): Target number of time frames for the output.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Mel spectrogram in dB scale, optionally resized to target_time_frames.\n",
    "    \"\"\"\n",
    "    if y is None or sr is None:\n",
    "        try:\n",
    "            y, sr = librosa.load(audio_path, sr=target_sr, res_type=Config.RES_TYPE)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file: {audio_path}, {e}\")\n",
    "            return None\n",
    "            \n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=y, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    if target_time_frames is not None:\n",
    "        mel_spectrogram_db_resized = librosa.util.fix_length(\n",
    "            mel_spectrogram_db, size=target_time_frames, axis=1)\n",
    "        return mel_spectrogram_db_resized\n",
    "    else:\n",
    "        return mel_spectrogram_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "The system uses EfficientNet-B0, a state-of-the-art convolutional neural network known for its efficiency and accuracy. The architecture includes:\n",
    "- Base EfficientNet-B0 network (without top layers)\n",
    "- Global Average Pooling layer\n",
    "- A dense layer with 128 units and ReLU activation\n",
    "- Output layer with sigmoid activation for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape=Config.INPUT_SHAPE):\n",
    "    \"\"\"\n",
    "    Build the EfficientNetB0 model architecture for inference.\n",
    "    \n",
    "    Args:\n",
    "        input_shape (tuple): Shape of the input tensor (height, width, channels).\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.models.Model: Compiled model architecture without weights.\n",
    "    \"\"\"\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    \n",
    "    # Load EfficientNetB0 base model (architecture) - WITHOUT weights initially\n",
    "    base_model = EfficientNetB0(include_top=False, weights=None, input_tensor=input_tensor)\n",
    "    \n",
    "    # Add Global Average Pooling layer\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Add a Dense layer for classification\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    output_tensor = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(weights_path=Config.MODEL_WEIGHTS_FILE):\n",
    "    \"\"\"\n",
    "    Load a trained model with weights for inference.\n",
    "    \n",
    "    Args:\n",
    "        weights_path (str): Path to the model weights file.\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.models.Model: Model with loaded weights.\n",
    "    \"\"\"\n",
    "    model = build_model()\n",
    "    try:\n",
    "        model.load_weights(weights_path)\n",
    "        print(f\"Model weights loaded from {weights_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model weights: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded from weights/capuchin_bird_classifier.weights.h5\n"
     ]
    }
   ],
   "source": [
    "model = load_trained_model()\n",
    "    \n",
    "if model is None:\n",
    "    print(\"Failed to load model. Exiting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Stage Sliding Window Algorithm\n",
    "\n",
    "The detection process employs a computationally efficient two-stage approach:\n",
    "\n",
    "1. **Stage 1 (Coarse Detection)**:\n",
    "   - Analyzes 6-second windows with minimal overlap\n",
    "   - Applies a higher threshold (0.6) to quickly reject windows without calls\n",
    "   - Only windows passing this threshold proceed to Stage 2\n",
    "\n",
    "2. **Stage 2 (Fine-Grained Detection)**:\n",
    "   - Only runs on promising segments identified in Stage 1\n",
    "   - Processes smaller chunks (0.3s) within the larger window\n",
    "   - Uses a lower threshold (0.5) for final detection\n",
    "   - Counts a call when at least one chunk in the window exceeds the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_capuchin_calls_two_stage_sliding_window(\n",
    "    long_audio_path, model=None,\n",
    "    threshold_stage1=Config.THRESHOLD_STAGE1, \n",
    "    threshold_stage2=Config.THRESHOLD_STAGE2,\n",
    "    window_duration_outer=Config.WINDOW_DURATION_OUTER,\n",
    "    step_duration_inner=Config.STEP_DURATION_INNER,\n",
    "    overlap_inner=Config.OVERLAP_INNER):\n",
    "    \"\"\"\n",
    "    Count Capuchin bird calls using a two-stage sliding window approach.\n",
    "    \n",
    "    This function implements an efficient detection algorithm that:\n",
    "    1. First uses a large window to quickly check entire segments\n",
    "    2. Then applies a finer-grained analysis only on promising segments\n",
    "    \n",
    "    Args:\n",
    "        long_audio_path (str): Path to the audio file to analyze.\n",
    "        model (tf.keras.models.Model): Trained model for prediction.\n",
    "        threshold_stage1 (float): Confidence threshold for the first stage detection.\n",
    "        threshold_stage2 (float): Confidence threshold for the second stage detection.\n",
    "        window_duration_outer (float): Duration of the outer sliding window in seconds.\n",
    "        step_duration_inner (float): Duration of inner chunks in seconds.\n",
    "        overlap_inner (float): Overlap fraction between inner chunks.\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of Capuchin bird calls detected.\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        print(\"Model is not provided. Please load your trained model.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        y_long, sr_long = librosa.load(long_audio_path, sr=Config.TARGET_SR, res_type=Config.RES_TYPE)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading long audio file: {long_audio_path}, {e}\")\n",
    "        return None\n",
    "\n",
    "    window_samples_outer = int(window_duration_outer * sr_long)\n",
    "    step_samples_inner = int(step_duration_inner * sr_long)\n",
    "    hop_samples_inner = int(step_samples_inner * (1 - overlap_inner))\n",
    "    total_duration_seconds = librosa.get_duration(y=y_long, sr=sr_long)\n",
    "\n",
    "    capuchin_call_count = 0\n",
    "    outer_window_start_time = 0.0\n",
    "\n",
    "    print(f\"Processing audio file (Two-Stage Sliding Window): {os.path.basename(long_audio_path)}\")\n",
    "    print(f\"Total duration: {total_duration_seconds:.2f} seconds\")\n",
    "    print(f\"Outer Window: {window_duration_outer}s, Stage 1 Threshold: {threshold_stage1}, Stage 2 Threshold: {threshold_stage2}\")\n",
    "    print(f\"Inner Step: {step_duration_inner}s, Inner Overlap: {overlap_inner}\")\n",
    "\n",
    "    # Outer sliding window loop (6-second windows)\n",
    "    outer_start_sample = 0\n",
    "    while outer_window_start_time < total_duration_seconds:\n",
    "        outer_end_sample = outer_start_sample + window_samples_outer\n",
    "        if outer_end_sample > len(y_long):\n",
    "            outer_end_sample = len(y_long)\n",
    "        outer_window_audio = y_long[outer_start_sample:outer_end_sample]\n",
    "        outer_window_duration = len(outer_window_audio) / sr_long\n",
    "        outer_window_end_time = outer_window_start_time + outer_window_duration\n",
    "\n",
    "        print(f\"\\nOuter Window: Start Time: {outer_window_start_time:.2f}s, End Time: {outer_window_end_time:.2f}s, Duration: {outer_window_duration:.2f}s\")\n",
    "\n",
    "        # Stage 1: Quick Check - Classify Entire 6-second Segment\n",
    "        mel_spec_outer_window = extract_mel_spectrogram(audio_path=None, y=outer_window_audio, sr=sr_long, target_time_frames=157)\n",
    "        stage1_predicted_class = 0  # Default to no call\n",
    "        if mel_spec_outer_window is not None:\n",
    "            mel_spec_outer_window_rgb = np.stack([mel_spec_outer_window] * 3, axis=-1)\n",
    "            mel_spec_outer_window_reshaped = mel_spec_outer_window_rgb[np.newaxis, ...]\n",
    "            stage1_prediction = model.predict(mel_spec_outer_window_reshaped, verbose=0)\n",
    "            stage1_prediction_prob = stage1_prediction[0][0]\n",
    "            stage1_predicted_class = int(stage1_prediction_prob > threshold_stage1)\n",
    "\n",
    "        if stage1_predicted_class == 0:  # No Call Indicated by Stage 1\n",
    "            print(\"  Stage 1: No Call Indicated - Skipping Stage 2\")\n",
    "        else:  # Call Indicated by Stage 1 - Proceed to Stage 2\n",
    "            print(\"  Stage 1: Call Indicated - Proceeding to Stage 2 Inner Loop\")\n",
    "            has_call_in_window = False\n",
    "            inner_start_sample = 0\n",
    "            # Stage 2: Detailed Analysis (Inner Loop - smaller chunks)\n",
    "            while inner_start_sample < len(outer_window_audio):\n",
    "                inner_end_sample = inner_start_sample + step_samples_inner\n",
    "                if inner_end_sample > len(outer_window_audio):\n",
    "                    inner_end_sample = len(outer_window_audio)\n",
    "                inner_chunk_audio = outer_window_audio[inner_start_sample:inner_end_sample]\n",
    "                inner_chunk_duration = len(inner_chunk_audio) / sr_long\n",
    "                inner_chunk_start_time_rel_outer = inner_start_sample / sr_long\n",
    "                inner_chunk_start_time_abs = outer_window_start_time + inner_chunk_start_time_rel_outer\n",
    "                inner_chunk_end_time_abs = inner_chunk_start_time_abs + inner_chunk_duration\n",
    "\n",
    "                if len(inner_chunk_audio) < step_samples_inner / 2:\n",
    "                    continue\n",
    "\n",
    "                mel_spec_chunk = extract_mel_spectrogram(audio_path=None, y=inner_chunk_audio, sr=sr_long)\n",
    "                if mel_spec_chunk is not None:\n",
    "                    mel_spec_chunk_rgb = np.stack([mel_spec_chunk] * 3, axis=-1)\n",
    "                    mel_spec_chunk_reshaped = mel_spec_chunk_rgb[np.newaxis, ...]\n",
    "                    prediction = model.predict(mel_spec_chunk_reshaped, verbose=0)\n",
    "                    prediction_prob_inner = prediction[0][0]\n",
    "                    predicted_class_inner = int(prediction_prob_inner > threshold_stage2)\n",
    "\n",
    "                    if predicted_class_inner == 1:\n",
    "                        has_call_in_window = True\n",
    "\n",
    "                inner_start_sample += hop_samples_inner\n",
    "\n",
    "            if has_call_in_window:\n",
    "                capuchin_call_count += 1\n",
    "                print(f\"  Call Event Detected in Outer Window - Incrementing Call Count\")\n",
    "            else:\n",
    "                print(f\"  Stage 2: No Call Event Detected in Outer Window (Despite Stage 1 Indication)\")\n",
    "        \n",
    "        outer_window_start_time += window_duration_outer\n",
    "        outer_start_sample = int(outer_window_start_time * sr_long)\n",
    "\n",
    "    print(f\"\\nTotal Capuchin calls detected in {os.path.basename(long_audio_path)}: {capuchin_call_count}\\n\")\n",
    "    return capuchin_call_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Processing\n",
    "\n",
    "The system processes audio files individually or in batches, calculating the number of Capuchin calls detected in each recording. Results can be saved to CSV files for further analysis.\n",
    "\n",
    "The output shown in this notebook demonstrates the detection process on a 3-minute recording (recording_06.mp3), successfully identifying 5 Capuchin bird calls at timestamps around 25s, 45s, 100s, 115s, and 130s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(audio_file_path, model):\n",
    "    \"\"\"\n",
    "    Process a single audio file to detect Capuchin bird calls.\n",
    "    \n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file.\n",
    "        model (tf.keras.models.Model): Trained model for prediction.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing the audio file name and the count of detected calls.\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarting inference on: {os.path.basename(audio_file_path)}\")\n",
    "    \n",
    "    call_count = count_capuchin_calls_two_stage_sliding_window(\n",
    "        long_audio_path=audio_file_path,\n",
    "        model=model\n",
    "    )\n",
    "    \n",
    "    result = {'audio_file': os.path.basename(audio_file_path), 'model_calls': call_count}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_files(audio_files, model):\n",
    "    \"\"\"\n",
    "    Process multiple audio files to detect Capuchin bird calls.\n",
    "    \n",
    "    Args:\n",
    "        audio_files (list): List of paths to audio files.\n",
    "        model (tf.keras.models.Model): Trained model for prediction.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing results for all processed files.\n",
    "    \"\"\"\n",
    "    print(f\"Processing {len(audio_files)} audio files...\")\n",
    "    print(\"\\nStarting inference (Multiple Files - Two-Stage Sliding Window)...\\n\")\n",
    "    \n",
    "    model_results = []\n",
    "    \n",
    "    for audio_file in audio_files:\n",
    "        print(f\"Processing audio file: {os.path.basename(audio_file)}\")\n",
    "        result = process_single_file(audio_file, model)\n",
    "        model_results.append(result)\n",
    "    \n",
    "    model_df = pd.DataFrame(model_results)\n",
    "    \n",
    "    print(\"\\nModel inference complete (Multiple Files).\")\n",
    "    print(model_df)\n",
    "    \n",
    "    return model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results_df, output_path=\"results/detection_results.csv\"):\n",
    "    \"\"\"\n",
    "    Save detection results to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pd.DataFrame): DataFrame containing detection results.\n",
    "        output_path (str): Path to save the CSV file.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        print(f\"Results saved to {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(results_df, output_path=\"results/detection_results.csv\"):\n",
    "    \"\"\"\n",
    "    Save detection results to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pd.DataFrame): DataFrame containing detection results.\n",
    "        output_path (str): Path to save the CSV file.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        results_df.to_csv(output_path, index=False)\n",
    "        print(f\"Results saved to {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files = [\n",
    "        os.path.join(Config.REAL_WORLD_DATA_FOLDER, f) \n",
    "        for f in os.listdir(Config.REAL_WORLD_DATA_FOLDER) \n",
    "        if f.lower().endswith(('.wav', '.mp3', '.flac', '.ogg'))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting inference on: recording_06.mp3\n",
      "Processing audio file (Two-Stage Sliding Window): recording_06.mp3\n",
      "Total duration: 180.04 seconds\n",
      "Outer Window: 6.0s, Stage 1 Threshold: 0.6, Stage 2 Threshold: 0.5\n",
      "Inner Step: 0.3s, Inner Overlap: 0.0\n",
      "\n",
      "Outer Window: Start Time: 0.00s, End Time: 6.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 6.00s, End Time: 12.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 12.00s, End Time: 18.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 18.00s, End Time: 24.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 24.00s, End Time: 30.00s, Duration: 6.00s\n",
      "  Stage 1: Call Indicated - Proceeding to Stage 2 Inner Loop\n",
      "  Call Event Detected in Outer Window - Incrementing Call Count\n",
      "\n",
      "Outer Window: Start Time: 30.00s, End Time: 36.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 36.00s, End Time: 42.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 42.00s, End Time: 48.00s, Duration: 6.00s\n",
      "  Stage 1: Call Indicated - Proceeding to Stage 2 Inner Loop\n",
      "  Call Event Detected in Outer Window - Incrementing Call Count\n",
      "\n",
      "Outer Window: Start Time: 48.00s, End Time: 54.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 54.00s, End Time: 60.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 60.00s, End Time: 66.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 66.00s, End Time: 72.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 72.00s, End Time: 78.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 78.00s, End Time: 84.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 84.00s, End Time: 90.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 90.00s, End Time: 96.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 96.00s, End Time: 102.00s, Duration: 6.00s\n",
      "  Stage 1: Call Indicated - Proceeding to Stage 2 Inner Loop\n",
      "  Call Event Detected in Outer Window - Incrementing Call Count\n",
      "\n",
      "Outer Window: Start Time: 102.00s, End Time: 108.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 108.00s, End Time: 114.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 114.00s, End Time: 120.00s, Duration: 6.00s\n",
      "  Stage 1: Call Indicated - Proceeding to Stage 2 Inner Loop\n",
      "  Call Event Detected in Outer Window - Incrementing Call Count\n",
      "\n",
      "Outer Window: Start Time: 120.00s, End Time: 126.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 126.00s, End Time: 132.00s, Duration: 6.00s\n",
      "  Stage 1: Call Indicated - Proceeding to Stage 2 Inner Loop\n",
      "  Call Event Detected in Outer Window - Incrementing Call Count\n",
      "\n",
      "Outer Window: Start Time: 132.00s, End Time: 138.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 138.00s, End Time: 144.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 144.00s, End Time: 150.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 150.00s, End Time: 156.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 156.00s, End Time: 162.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 162.00s, End Time: 168.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 168.00s, End Time: 174.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 174.00s, End Time: 180.00s, Duration: 6.00s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Outer Window: Start Time: 180.00s, End Time: 180.04s, Duration: 0.04s\n",
      "  Stage 1: No Call Indicated - Skipping Stage 2\n",
      "\n",
      "Total Capuchin calls detected in recording_06.mp3: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "single_file = audio_files[6]  # Change index to select a different file\n",
    "single_result = process_single_file(single_file, model)\n",
    "single_df = pd.DataFrame([single_result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Considerations\n",
    "\n",
    "The two-stage approach significantly reduces computational requirements compared to a single-stage sliding window method. By quickly filtering out audio segments that are unlikely to contain bird calls (Stage 1), the system minimizes the need for more intensive processing (Stage 2), making it suitable for analyzing large volumes of audio data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
